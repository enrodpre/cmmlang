from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

import pytest

TEST_BUILD_DIR = Path("/home/kike/dev/cmm/build/test")
BINARY_PATH = TEST_BUILD_DIR / "CmmLang"
EXAMPLES_PATH = TEST_BUILD_DIR / "examples"
GENERATED_PATH = TEST_BUILD_DIR / "generated"

OPTION_EXAMPLE_FILE = "--example-file"

DEFAULT_OK_STATUS = 255


class Color:
    ERROR = "\033[1;38;2;205;92;92m"
    GREEN = "\033[92m"
    RED = "\033[91m"
    YELLOW = "\033[93m"
    RESET = "\033[0m"


def colorize(color: Color, payload: str) -> str:
    return f"{color}{payload}{Color.RESET}"


@dataclass
class TestResult:
    nodeid: str
    outcome: str
    duration: float
    error_message: Optional[str] = None


class TestDataCollector:
    """Singleton to collect test data during run"""

    def __init__(self):
        self.results = []
        self.session_data = {}

    def add_result(self, result: TestResult):
        self.results.append(result)

    def get_results(self) -> List[TestResult]:
        return self.results

    def set_session_data(self, key: str, value: Any):
        self.session_data[key] = value

    def get_session_data(self) -> Dict[str, Any]:
        return self.session_data


collector = TestDataCollector()


@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    outcome = yield
    rep = outcome.get_result()
    if rep.when == "call":
        test_result = TestResult(
            nodeid=item.nodeid,
            outcome=call.excinfo is None and "PASSED" or "FAILED",
            duration=call.duration,
            error_message=str(call.excinfo.value) if call.excinfo else None,
        )
        collector.add_result(test_result)

        output = ""
        match (None):
            case 0:
                output += colorize(Color.GREEN, "OK")
            case 1:
                output += colorize(Color.RED, "BINARY NOT FOUND")
            case 2:
                output += colorize(Color.RED, "TESTFILE NOT FOUND")
            case 3:
                output += colorize(
                    Color.ERROR, f"COMPILATION ERROR: {test_result.error_message}"
                )
            case 4:
                output += colorize(
                    Color.ERROR, f"RUN ERROR: {test_result.error_message}"
                )
            case 5:
                output += colorize(
                    Color.ERROR,
                    f"ASSERTION ERROR: EXPECTED 255, ACTUAL {outcome}",
                )

        print(test_result.error_message)


class BinaryNotFound(Exception):
    def __init__(self, path: str):
        self.path = path


class TestfileNotFound(Exception):
    def __init__(self, path: str):
        self.path = path


class CompilationError(Exception):
    def __init__(self, message: str):
        self.message = message


class RunError(Exception):
    def __init__(self, message: str):
        self.message = message


class AssertionError(Exception):
    def __init__(self, exit_code: int):
        self.exit_code = exit_code


def pytest_exception_interact(node, call, report):
    exc = call.excinfo.value
    match (exc):
        case BinaryNotFound():
            report.longrepr = f"[{colorize(Color.YELLOW,call.excinfo.value.__class__.__name__)}] Binary not found in {exc.path}"
        case TestfileNotFound():
            report.longrepr = f"[{colorize(Color.YELLOW,call.excinfo.value.__class__.__name__)}] Test file not found in {exc.path}"
        case CompilationError():
            report.longrepr = f"[{colorize(Color.ERROR, call.excinfo.value.__class__.__name__)}] Compilation error: {exc.message}"
        case RunError():
            report.longrepr = f"[{colorize(Color.ERROR, call.excinfo.value.__class__.__name__)}] Run error: {exc.message}"
        case AssertionError():
            report.longrepr = f"[{colorize(Color.ERROR, call.excinfo.value.__class__.__name__)}] Expected 255, actual {exc.exit_code}"


def pytest_sessionfinish(session, exitstatus):
    """Hook called after all tests complete"""

    # if collector:
    #     print("\n" + "=" * 50)
    #     print("INTEGRATION TEST SUMMARY")
    #     print("=" * 50)
    #
    #     passed = sum(
    #         1
    #         for (filename, status, exit_status, message) in collector
    #         if status == test_status.OK
    #     )
    #     failed = sum(
    #         1
    #         for (filename, status, exit_status, message) in collector
    #         if status
    #         in (
    #             test_status.BINARY_NOT_FOUND,
    #             test_status.TESTFILE_NOT_FOUND,
    #             test_status.COMPILATION_ERROR,
    #             test_status.RUN_ERROR,
    #             test_status.ASSERTION_ERROR,
    #         )
    #     )
    #     skipped = sum(
    #         1
    #         for (filename, status, exit_status, message) in collector
    #         # if result == "skipped"
    #     )
    #
    #     print(
    #         f"Total: {len(collector)} | "
    #         f"{Color.GREEN}Passed: {passed}{Color.RESET} | "
    #         f"{Color.RED}Failed: {failed}{Color.RESET} | "
    #         f"{Color.YELLOW}Skipped: {skipped}{Color.RESET}"
    #     )
    #     print("=" * 50)


def pytest_configure(config):
    config.option.quiet = 2  # -qq level
    config.option.tb = "no"
    config.option.disable_warnings = True
    config.option.show_capture = "no"


def pytest_report_header(config):
    """Suppress pytest header"""
    return None


def pytest_collection_modifyitems(config, items):
    """Suppress collection output"""
    pass


def pytest_terminal_summary(terminalreporter, exitstatus, config):
    """Override terminal summary to prevent default output"""
    # This prevents the default summary from being printed
    pass


@pytest.hookimpl(tryfirst=True)
def pytest_runtest_logstart(nodeid, location):
    # Suppress test start logs
    pass


# ------------------------------
# CLI option
# ------------------------------
def pytest_addoption(parser):
    parser.addoption(
        OPTION_EXAMPLE_FILE,
        action="store",
        default=None,
        help="Run tests only on a specific example file",
    )


def read_examples(base_path: Path) -> list[Path]:
    return [filepath for filepath in base_path.iterdir() if filepath.is_file()]


def pytest_sessionstart(session):
    for p in GENERATED_PATH.iterdir():
        if p.is_file():
            p.unlink()


def pytest_generate_tests(metafunc):
    """Dynamically parametrize test_example() with example files."""
    if "example_file" in metafunc.fixturenames:
        cli_file = metafunc.config.getoption(OPTION_EXAMPLE_FILE)

        files = None
        if cli_file:
            files = [Path(cli_file)]
        else:
            files = read_examples(Path("examples"))

        metafunc.parametrize("example_file", files)
    else:
        print("error")
